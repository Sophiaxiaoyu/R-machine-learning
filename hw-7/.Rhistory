covmat_quad_big_vague <- solve(solve(B0_vague) + (t(Xmat_quad_big) %*% Xmat_quad_big)/ 1^2)
covmat_quad_small_inform %>% diag() %>% sqrt()
# the result from 1f)
covmat_quad_small_noprior %>% diag() %>% sqrt()
covmat_quad_big_inform %>% diag() %>% sqrt()
# the result from 1f)
covmat_quad_big_noprior %>% diag() %>% sqrt()
covmat_quad_small_vague %>% diag() %>% sqrt()
# the result from 1f)
covmat_quad_small_noprior %>% diag() %>% sqrt()
covmat_quad_big_vague %>% diag() %>% sqrt()
# the result from 1f)
covmat_quad_big_noprior %>% diag() %>% sqrt()
info_quad <- list(
yobs = quad_data_big$y,
design_matrix = Xmat_quad_big,
mu_beta = 0,
tau_beta = 3,
sigma_rate = 1
)
lm_logpost <- function(unknowns, my_info)
{
# specify the number of unknown beta parameters
length_beta <- ncol(my_info$design_matrix)
# extract the beta parameters from the `unknowns` vector
beta_v <- unknowns[1 : length_beta]
# extract the unbounded noise parameter, varphi
lik_varphi <- unknowns[length_beta + 1]
# back-transform from varphi to sigma
lik_sigma <- exp(lik_varphi)
# extract design matrix
X <- my_info$design_matrix
# calculate the linear predictor
mu <- as.vector(X %*% as.matrix(beta_v))
# evaluate the log-likelihood
log_lik <- sum(dnorm(x = my_info$yobs,
mean = mu,
sd = lik_sigma,
log = TRUE))
# evaluate the log-prior
log_prior_beta <- sum(dnorm(x = beta_v,
mean = my_info$mu_beta,
sd = my_info$tau_beta,
log = TRUE))
log_prior_sigma <- dexp(x = lik_sigma,
rate = my_info$sigma_rate,
log = TRUE)
# add the mean trend prior and noise prior together
log_prior <- log_prior_beta + log_prior_sigma
# account for the transformation
log_derive_adjust <- lik_varphi
# sum together
log_lik + log_prior + log_derive_adjust
}
lm_logpost(rep(-1, ncol(Xmat_quad_big) + 1), info_quad)
lm_logpost(rep(1, ncol(Xmat_quad_big) + 1), info_quad)
my_laplace <- function(start_guess, logpost_func, ...)
{
# code adapted from the `LearnBayes`` function `laplace()`
fit <- optim(start_guess,
logpost_func,
gr = NULL,
...,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
mode <- fit$par
post_var_matrix <- -solve(fit$hessian)
p <- length(mode) # number of unknown parameters
int <- p/2 * log(2 * pi) + 0.5 * log(det(post_var_matrix)) + logpost_func(mode, ...)
# package all of the results into a list
list(mode = mode,
var_matrix = post_var_matrix,
log_evidence = int,
converge = ifelse(fit$convergence == 0,
"YES",
"NO"),
iter_counts = as.numeric(fit$counts[1]))
}
laplace_quad <- my_laplace(rep(0, ncol(Xmat_quad_big)+1), lm_logpost, info_quad)
laplace_quad$mode
laplace_quad$var_matrix %>% diag() %>% sqrt()
generate_lm_post_samples <- function(mvn_result, length_beta, num_samples)
{
MASS::mvrnorm(n = num_samples,
mu = mvn_result$mode  ,
Sigma = mvn_result$var_matrix  ) %>%
as.data.frame() %>% tibble::as_tibble() %>%
purrr::set_names(c(sprintf("beta_%02d", 0:(length_beta-1)), "varphi")) %>%
# back transform varphi to sigma!!!
mutate(sigma =  exp(varphi))
}
set.seed(87123)
post_samples_quad <- generate_lm_post_samples(laplace_quad, ncol(Xmat_quad_big), 2500)
post_samples_quad %>%
ggplot(mapping = aes(x = beta_02)) +
geom_histogram(bins = 55)
mean(post_samples_quad$beta_02 > 0)
post_lm_pred_trend_samples <- function(Xnew, Bmat)
{
# matrix of posterior trend predictions
Umat <- Xnew %*% t(Bmat)
# return Umat, completed for you
Umat
}
make_post_lm_pred <- function(Xnew, post)
{
Bmat <- post %>% select(starts_with("beta_")) %>% as.matrix()
sigma_vector <- post %>% pull(sigma)
post_lm_pred_trend_samples(Xnew, Bmat)
}
post_trend_samples_quad <- make_post_lm_pred(Xmat_quad_big, post_samples_quad)
post_trend_samples_quad %>% dim()
### error of the first posterior sample
error_quad_post_01 <- [, 1] - quad_data_big$y
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
quad_data_big <- readr::read_csv('quad_demo_data.csv', col_names = TRUE)
quad_data_big %>% glimpse()
quad_data_small <- quad_data_big %>% slice(1:5)
quad_data_small
quad_data_big %>% tibble::rowid_to_column() %>%
ggplot(mapping = aes(x = x, y = y)) +
geom_point(mapping = aes(color = rowid > 5),
size = 3)
### make the design matrix for the SMALL data
Xmat_quad_small <- model.matrix(y ~ x + I(x^2), data = quad_data_small)
### make the design matrix for the BIG data
Xmat_quad_big <- model.matrix(y ~ x + I(x^2), data = quad_data_big)
data.frame(
n_rows_Xmat_quad_small = nrow(Xmat_quad_small),
n_cols_Xmat_quad_small = ncol(Xmat_quad_small)
)
data.frame(
n_rows_Xmat_quad_bid = nrow(Xmat_quad_big),
n_cols_Xmat_quad_big = ncol(Xmat_quad_big)
)
Xmat_quad_small
### posterior covariance matrix for the SMALL data
covmat_quad_small_noprior <- 1^2 * solve(t(Xmat_quad_small) %*% Xmat_quad_small)
### posterior covariance matrix for the BIG data
covmat_quad_big_noprior <- 1^2 * solve(t(Xmat_quad_big) %*% Xmat_quad_big)
data.frame(
n_rows = nrow(covmat_quad_small_noprior),
n_cols = ncol(covmat_quad_small_noprior)
)
data.frame(
n_rows = nrow(covmat_quad_big_noprior),
n_cols = ncol(covmat_quad_big_noprior)
)
covmat_quad_small_noprior %>% diag() %>% sqrt()
covmat_quad_big_noprior %>% diag() %>% sqrt()
y_quad_big_col <- quad_data_big %>% pull(y) %>% as.matrix()
y_quad_small_col <- quad_data_small %>% pull(y) %>% as.matrix()
y_quad_small_col
### posterior mean vector for the SMALL data
postmeans_quad_small_noprior <- solve( t(Xmat_quad_small) %*% Xmat_quad_small ,
t(Xmat_quad_small) %*% y_quad_small_col)
### posterior mean vector for the BIG data
postmeans_quad_big_noprior <- solve( t(Xmat_quad_big) %*% Xmat_quad_big,
t(Xmat_quad_big) %*% y_quad_big_col)
postmeans_quad_small_noprior
postmeans_quad_big_noprior
data.frame(
coefficient_name = colnames(Xmat_quad_small),
post_lwr = as.vector(postmeans_quad_small_noprior - 2 * sqrt(diag(covmat_quad_small_noprior))),
post_upr = as.vector(postmeans_quad_small_noprior + 2 * sqrt(diag(covmat_quad_small_noprior)))
)
data.frame(
coefficient_name = colnames(Xmat_quad_small),
post_lwr = as.vector(postmeans_quad_big_noprior - 2 * sqrt(diag(covmat_quad_small_noprior))),
post_upr = as.vector(postmeans_quad_big_noprior + 2 * sqrt(diag(covmat_quad_small_noprior)))
)
### prior covariance matrix for the INFORMATIVE prior
B0_inform <- 1.4^2 * diag(ncol(Xmat_quad_small))
### prior covariance matrix for the VAGUE prior
B0_vague <- 30^2 * diag(ncol(Xmat_quad_small))
B0_inform
B0_vague
### posterior covariance matrix INFORMATIVE prior and SMALL data
covmat_quad_small_inform <- solve(solve(B0_inform) + (t(Xmat_quad_small) %*% Xmat_quad_small)/ 1^2)
### posterior covariance matrix INFORMATIVE prior and BIG data
covmat_quad_big_inform <- solve(solve(B0_inform) + (t(Xmat_quad_big) %*% Xmat_quad_big)/ 1^2)
### posterior covariance matrix VAGUE prior and SMALL data
covmat_quad_small_vague <- solve(solve(B0_vague) + (t(Xmat_quad_small) %*% Xmat_quad_small)/ 1^2)
### posterior covariance matrix VAGUE prior and BIG data
covmat_quad_big_vague <- solve(solve(B0_vague) + (t(Xmat_quad_big) %*% Xmat_quad_big)/ 1^2)
covmat_quad_small_inform %>% diag() %>% sqrt()
# the result from 1f)
covmat_quad_small_noprior %>% diag() %>% sqrt()
covmat_quad_big_inform %>% diag() %>% sqrt()
# the result from 1f)
covmat_quad_big_noprior %>% diag() %>% sqrt()
covmat_quad_small_vague %>% diag() %>% sqrt()
# the result from 1f)
covmat_quad_small_noprior %>% diag() %>% sqrt()
covmat_quad_big_vague %>% diag() %>% sqrt()
# the result from 1f)
covmat_quad_big_noprior %>% diag() %>% sqrt()
info_quad <- list(
yobs = quad_data_big$y,
design_matrix = Xmat_quad_big,
mu_beta = 0,
tau_beta = 3,
sigma_rate = 1
)
lm_logpost <- function(unknowns, my_info)
{
# specify the number of unknown beta parameters
length_beta <- ncol(my_info$design_matrix)
# extract the beta parameters from the `unknowns` vector
beta_v <- unknowns[1 : length_beta]
# extract the unbounded noise parameter, varphi
lik_varphi <- unknowns[length_beta + 1]
# back-transform from varphi to sigma
lik_sigma <- exp(lik_varphi)
# extract design matrix
X <- my_info$design_matrix
# calculate the linear predictor
mu <- as.vector(X %*% as.matrix(beta_v))
# evaluate the log-likelihood
log_lik <- sum(dnorm(x = my_info$yobs,
mean = mu,
sd = lik_sigma,
log = TRUE))
# evaluate the log-prior
log_prior_beta <- sum(dnorm(x = beta_v,
mean = my_info$mu_beta,
sd = my_info$tau_beta,
log = TRUE))
log_prior_sigma <- dexp(x = lik_sigma,
rate = my_info$sigma_rate,
log = TRUE)
# add the mean trend prior and noise prior together
log_prior <- log_prior_beta + log_prior_sigma
# account for the transformation
log_derive_adjust <- lik_varphi
# sum together
log_lik + log_prior + log_derive_adjust
}
lm_logpost(rep(-1, ncol(Xmat_quad_big) + 1), info_quad)
lm_logpost(rep(1, ncol(Xmat_quad_big) + 1), info_quad)
my_laplace <- function(start_guess, logpost_func, ...)
{
# code adapted from the `LearnBayes`` function `laplace()`
fit <- optim(start_guess,
logpost_func,
gr = NULL,
...,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
mode <- fit$par
post_var_matrix <- -solve(fit$hessian)
p <- length(mode) # number of unknown parameters
int <- p/2 * log(2 * pi) + 0.5 * log(det(post_var_matrix)) + logpost_func(mode, ...)
# package all of the results into a list
list(mode = mode,
var_matrix = post_var_matrix,
log_evidence = int,
converge = ifelse(fit$convergence == 0,
"YES",
"NO"),
iter_counts = as.numeric(fit$counts[1]))
}
laplace_quad <- my_laplace(rep(0, ncol(Xmat_quad_big)+1), lm_logpost, info_quad)
laplace_quad$mode
laplace_quad$var_matrix %>% diag() %>% sqrt()
generate_lm_post_samples <- function(mvn_result, length_beta, num_samples)
{
MASS::mvrnorm(n = num_samples,
mu = mvn_result$mode  ,
Sigma = mvn_result$var_matrix  ) %>%
as.data.frame() %>% tibble::as_tibble() %>%
purrr::set_names(c(sprintf("beta_%02d", 0:(length_beta-1)), "varphi")) %>%
# back transform varphi to sigma!!!
mutate(sigma =  exp(varphi))
}
set.seed(87123)
post_samples_quad <- generate_lm_post_samples(laplace_quad, ncol(Xmat_quad_big), 2500)
post_samples_quad %>%
ggplot(mapping = aes(x = beta_02)) +
geom_histogram(bins = 55)
mean(post_samples_quad$beta_02 > 0)
post_lm_pred_trend_samples <- function(Xnew, Bmat)
{
# matrix of posterior trend predictions
Umat <- Xnew %*% t(Bmat)
# return Umat, completed for you
Umat
}
make_post_lm_pred <- function(Xnew, post)
{
Bmat <- post %>% select(starts_with("beta_")) %>% as.matrix()
sigma_vector <- post %>% pull(sigma)
post_lm_pred_trend_samples(Xnew, Bmat)
}
post_trend_samples_quad <- make_post_lm_pred(Xmat_quad_big, post_samples_quad)
post_trend_samples_quad %>% dim()
### error of the first posterior sample
error_quad_post_01 <- [, 1] - quad_data_big$y
### error of the first posterior sample
error_quad_post_01 <- post_trend_samples_quad[, 1] - quad_data_big$y
### error of the second posterior sample
error_quad_post_02 <- post_trend_samples_quad[, 2] - quad_data_big$y
### error of the third posterior sample
error_quad_post_03 <- post_trend_samples_quad[, 3] - quad_data_big$y
length(error_quad_post_01)
mae_quad_post_01 <- mean(abs(error_quad_post_01))
mae_quad_post_02 <- mean(abs(error_quad_post_02))
mae_quad_post_03 <- mean(abs(error_quad_post_03))
mae_quad_post_01
mae_quad_post_02
mae_quad_post_03
knitr::opts_chunk$set(echo = TRUE)
### a 2 x 4 matrix
matrix(1:8, nrow = 2, byrow = TRUE)
### a vector length 2
c(1, 2)
### subtracting the two yields a matrix
matrix(1:8, nrow = 2, byrow = TRUE) - c(1, 2)
absError_quad_mat <-
### dimensions?
absError_quad_mat <- abs( post_trend_samples_quad - quad_data_big$y )
### dimensions?
absError_quad_mat %>% dim()
MAE_quad <- colMeans( absError_quad_mat )
### data type?
class(MAE_quad)
### length?
length(MAE_quad)
MAE_quad[1:3] == c(mae_quad_post_01, mae_quad_post_02, mae_quad_post_03)
MAE_quad %>% quantile()
file_for_train <- "hw07_train_data.csv"
train_df <- readr::read_csv(file_for_train, col_names = TRUE)
file_for_test <- "hw07_test_data.csv"
test_df <- readr::read_csv(file_for_test, col_names = TRUE)
train_df %>% glimpse()
test_df %>% glimpse()
train_df %>%
mutate(dataset = "trainingset") %>%
bind_rows(test_df %>%
mutate(dataset = "testset")) %>%
ggplot(mapping = aes(x = x, y = y)) +
geom_point(mapping = aes(color = dataset),
size = 2)
train_df %>%
mutate(dataset = "trainingset") %>%
bind_rows(test_df %>% mutate(dataset = "testset")) %>%
ggplot(mapping = aes(x = x, y = y)) +
geom_point(mapping = aes(color = dataset))
make_spline_basis_mats <- function(J, train_data, test_data)
{
train_basis <- splines::ns( )
interior_knots <- as.vector(attributes(train_basis)$knots)
boundary_knots <- as.vector(attributes(train_basis)$Boundary.knots)
train_matrix <- model.matrix( )
test_matrix <- model.matrix( )
tibble::tibble(
design_matrix = list(train_matrix),
test_matrix = list(test_matrix)
)
}
make_spline_basis_mats <- function(J, train_data, test_data)
{
train_basis <- splines::ns( train_data$x, df = J )
interior_knots <- as.vector(attributes(train_basis)$knots)
boundary_knots <- as.vector(attributes(train_basis)$Boundary.knots)
train_matrix <- model.matrix( ~ splines::ns(x, knots = knots_use_basis), data = train_data)
test_matrix <- model.matrix( ~ splines::ns(x, knots = knots_use_basis), data = test_data )
tibble::tibble(
design_matrix = list(train_matrix),
test_matrix = list(test_matrix)
)
}
make_spline_basis_mats <- function(J, train_data, test_data)
{
train_basis <- splines::ns( train_data$x, df = J )
interior_knots <- as.vector(attributes(train_basis)$knots)
boundary_knots <- as.vector(attributes(train_basis)$Boundary.knots)
train_matrix <- model.matrix( ~ splines::ns(x, knots = knots_use_basis), data = train_data)
test_matrix <- model.matrix( ~ splines::ns(x, knots = knots_use_basis), data = test_data )
tibble::tibble(
design_matrix = list(train_matrix),
test_matrix = list(test_matrix)
)
}
spline_matrices <- purrr::map_dfr(seq(1, 50, by = 2),
make_spline_basis_mats,
train_data = train_df,
test_data = test_df)
make_spline_basis_mats <- function(J, train_data, test_data)
{
train_basis <- splines::ns(train_data$x, df = J)
knots_use_basis <- as.vector(attributes(train_basis)$knots)
train_matrix <- model.matrix( ~ splines::ns(x, knots = knots_use_basis), data = train_data)
test_matrix <- model.matrix( ~ splines::ns(x, knots = knots_use_basis), data = test_data)
tibble::tibble(
design_matrix = list(train_matrix),
test_matrix = list(test_matrix)
)
}
spline_matrices <- purrr::map_dfr(seq(1, 50, by = 2),
make_spline_basis_mats,
train_data = train_df,
test_data = test_df)
glimpse(spline_matrices)
make_spline_basis_mats <- function(J, train_data, test_data)
{
train_basis <- splines::ns(train_data$x, df = J)
interior_knots <- as.vector(attributes(train_basis)$knots)
boundary_knots <- as.vector(attributes(train_basis)$Boundary.knots)
train_matrix <- model.matrix( ~  splines::ns(x, knots = interior_knots), data = train_data)
test_matrix <- model.matrix( ~  splines::ns(x, knots = interior_knots), data = test_data)
tibble::tibble(
design_matrix = list(train_matrix),
test_matrix = list(test_matrix)
)
}
spline_matrices <- purrr::map_dfr(seq(1, 50, by = 2),
make_spline_basis_mats,
train_data = train_df,
test_data = test_df)
glimpse(spline_matrices)
spline_matrices
dim(spline_matrices$design_matrix[[1]])
dim(spline_matrices$test_matrix[[1]])
dim(spline_matrices$design_matrix[[2]])
dim(spline_matrices$test_matrix[[2]])
info_splines_train <- list(
yobs = train_df$y,
mu_beta = 0,
tau_beta = 20,
sigma_rate = 1
)
manage_spline_fit <- function(Xtrain, logpost_func, my_settings)
{
my_settings$design_matrix <- Xtrain
init_beta <- rnorm(ncol(Xtrain))
init_varphi <- log(rexp(n = 1, rate = my_settings$sigma_rate))
my_laplace(c(init_beta, init_varphi), logpost_func, my_settings )
}
set.seed(724412)
all_spline_models <- purrr::map(spline_matrices$design_matrix,
manage_spline_fit,
logpost_func = lm_logpost,
my_settings = info_splines_train)
all_spline_models[[2]]
purrr::map_chr(all_spline_models, "converge")
calc_mae_from_laplace <- function(mvn_result, Xtrain, Xtest, y_train, y_test, num_samples)
{
# generate posterior samples from the approximate MVN posterior
post <- generate_lm_post_samples(mvn_result, ncol(Xtrain), num_samples)
# make posterior predictions on the training set
pred_train <- make_post_lm_pred(Xtrain, post)
# make posterior predictions on the test set
pred_test <- make_post_lm_pred(Xtest, post)
# calculate the error between the training set predictions
# and the training set observations
error_train <- pred_train - y_train
# calculate the error between the test set predictions
# and the test set observations
error_test <- pred_test - y_test
# calculate the MAE on the training set
mae_train <- colMeans(abs(error_train))
# calculate the MAE on the test set
mae_test <- colMeans(abs(error_test))
# book keeping, package together the results
mae_train_df <- tibble::tibble(
mae = mae_train
) %>%
mutate(dataset = "training") %>%
tibble::rowid_to_column("post_id")
mae_test_df <- tibble::tibble(
mae = mae_test
) %>%
mutate(dataset = "test") %>%
tibble::rowid_to_column("post_id")
# you MUST specify the order, J, associated with the spline model
mae_train_df %>%
bind_rows(mae_test_df) %>%
mutate(J = ncol(Xtrain) - 1)
}
set.seed(52133)
all_spline_mae_results <- purrr::pmap_dfr(list(all_spline_models,
spline_matrices$design_matrix,
spline_matrices$test_matrix),
calc_mae_from_laplace,
y_train = train_df$y,
y_test = test_df$y,
num_samples = 2500)
all_spline_mae_results %>% glimpse()
all_spline_mae_results %>%
ggplot(mapping = aes(x = as.factor(J), y = mae)) +
geom_boxplot(mapping = aes(fill = dataset)) +
scale_fill_brewer(palette = "Set1")
all_spline_mae_results %>%
ggplot(mapping = aes(x = J, y = mae)) +
stat_summary(geom = 'line', fun = 'median',
mapping = aes(color = dataset),
size = 1.2) +
scale_color_brewer(palette = 'Set1')
spline_weights <- exp(spline_evidence) / sum(exp(spline_evidence))
spline_evidence <- purrr::map_dbl(all_spline_models, "log_evidence")
spline_weights <- exp(spline_evidence) / sum(exp(spline_evidence))
spline_dof <- seq(1, 50, by = 2)
tibble::tibble(
w = spline_weights
) %>%
mutate(J = spline_dof) %>%
ggplot(mapping = aes(x = as.factor(J), y = w))+
geom_bar(stat = "identity")
